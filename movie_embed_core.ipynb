{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import functools\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "train_url = 'https://download.mlcc.google.com/mledu-datasets/sparse-data-embedding/train.tfrecord'\n",
    "train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)\n",
    "test_url = 'https://download.mlcc.google.com/mledu-datasets/sparse-data-embedding/test.tfrecord'\n",
    "test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the vocabulary file.\n",
    "terms_url = 'https://download.mlcc.google.com/mledu-datasets/sparse-data-embedding/terms.txt'\n",
    "terms_path = tf.keras.utils.get_file(terms_url.split('/')[-1], terms_url)\n",
    "\n",
    "vocab = None\n",
    "with io.open(terms_path, 'r', encoding='utf8') as f:\n",
    "    vocab = list(set(f.read().split()))\n",
    "\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def parse_fn(record):\n",
    "    features = {\n",
    "        \"terms\": tf.VarLenFeature(dtype=tf.string), # terms are strings of varying lengths\n",
    "        \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32) # labels are 0 or 1\n",
    "    }\n",
    "  \n",
    "    parsed_features = tf.parse_single_example(record, features)\n",
    "\n",
    "    terms = parsed_features['terms'].values\n",
    "    labels = parsed_features['labels']\n",
    "\n",
    "    return  terms, labels\n",
    "\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "ds = tf.data.TFRecordDataset(filenames)\n",
    "ds = ds.map(parse_fn)\n",
    "ds = ds.padded_batch(LARGE, ds.output_shapes)\n",
    "ds_iter = ds.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(ds_iter.initializer, feed_dict={filenames: [train_path]})\n",
    "    x_train, y_train = sess.run(ds_iter.get_next())\n",
    "\n",
    "    sess.run(ds_iter.initializer, feed_dict={filenames: [test_path]})\n",
    "    x_test, y_test = sess.run(ds_iter.get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data\n",
    "\n",
    "Specifically need to replace each word with a vocabulary index; pad with '-1' so not jagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data\n",
    "@functools.lru_cache(maxsize=None, typed=False)\n",
    "def get_word_index(word):\n",
    "    i = bisect.bisect_left(vocab, word)\n",
    "    if i != len(vocab) and vocab[i] == word:\n",
    "        return i\n",
    "\n",
    "    \n",
    "def convert_words_to_index(words):\n",
    "    idx = []\n",
    "    for word in words:\n",
    "        if word == b'':\n",
    "            continue\n",
    "        i = get_word_index(word.decode('utf-8'))\n",
    "        if i:\n",
    "            idx.append(i)\n",
    "    idx += [-1] * (len(words) - len(idx))\n",
    "    return idx\n",
    "\n",
    "\n",
    "x_train = np.array([convert_words_to_index(words) for words in x_train])\n",
    "x_test = np.array([convert_words_to_index(words) for words in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_SIZE = 2\n",
    "\n",
    "x = tf.sparse_placeholder(tf.int32)\n",
    "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "embed_matrix = tf.get_variable('embed_matrix', \n",
    "                               shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                               initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "embed = tf.nn.embedding_lookup_sparse(embed_matrix, x, None, name='embedding')\n",
    "\n",
    "hidden_layer1 = tf.layers.Dense(units=10)\n",
    "hidden_layer2 = tf.layers.Dense(units=10)\n",
    "logit_layer = tf.layers.Dense(units=2)\n",
    "\n",
    "hidden1 = hidden_layer1(embed)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "logits = logit_layer(hidden2)\n",
    "\n",
    "pred_class = tf.argmax(logits, 1)\n",
    "pred_prob = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inputs\n",
    "def input_fn(x, y, batch_size=1, shuffle=True, num_epochs=None):    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=LARGE)\n",
    "    \n",
    "    ds = ds.repeat(num_epochs).batch(batch_size)\n",
    "    return ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "\n",
    "train_iterator = input_fn(x_train, y_train, batch_size=25, shuffle=True, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.81204\n",
      "AUC:  0.81204\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.81      0.81     12500\n",
      "        1.0       0.81      0.82      0.81     12500\n",
      "\n",
      "avg / total       0.81      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "def np_to_sparse_tensor(arr, null_idx=-1):\n",
    "    idx  = np.where(arr != null_idx)\n",
    "    return tf.SparseTensorValue(np.vstack(idx).T, arr[idx], arr.shape)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    var_init = tf.global_variables_initializer()\n",
    "    sess.run(var_init)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            xi_train, yi_train = sess.run(train_iterator)\n",
    "            sess.run(train_op, feed_dict={x: np_to_sparse_tensor(xi_train), y: yi_train})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # evaluate\n",
    "    y_test_pred = sess.run(pred_class, feed_dict={x: np_to_sparse_tensor(x_test), y: y_test})\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"AUC: \", roc_auc_score(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
